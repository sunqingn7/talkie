app:
  name: "Talkie Voice Assistant"
  version: "1.0.0"
  debug: false

audio:
  sample_rate: 16000
  chunk_size: 4096
  channels: 1
  format: "int16"
  
stt:
  primary: "whisper"  # whisper only (using whisper.cpp server)
  whisper:
    server_url: "http://localhost:8081"  # whisper.cpp HTTP server URL
    model: "base.en"  # Must match the model loaded in whisper-server
    language: "en"
    binary_path: "/home/qing/Project/whisper.cpp/build/bin/whisper-server"
    model_path: "/home/qing/Project/talkie/models/ggml-base.en.bin"
    
tts:
  engine: "coqui"  # coqui, pyttsx3, gtts, edge
  # Coqui TTS settings (recommended - high quality)
  
  # IMPORTANT: Language-specific models can only speak their trained language!
  # - English model: Can only speak English (will discard Chinese characters)
  # - Chinese model: Can only speak Chinese (will discard English letters)
  # 
  # To speak BOTH English and Chinese smoothly, use XTTS-v2 (requires license):
  
  # OPTION 1: XTTS-v2 (RECOMMENDED for multilingual - 17 languages!)
  # Supports: English, Chinese, Spanish, French, German, Italian, Portuguese, 
  #           Polish, Turkish, Russian, Dutch, Czech, Arabic, Japanese, 
  #           Hungarian, Korean, Hindi
  # Steps to enable:
  #   1. Set environment variable: export COQUI_TOS_AGREED=1
  #   2. Uncomment the line below:
  # coqui_model: "tts_models/multilingual/multi-dataset/xtts_v2"
  #   3. Restart the assistant
  # Model size: ~1.5GB
  # License: Non-commercial use only (CPML) unless you purchase commercial license
  
  # OPTION 2: Current setup (separate models, auto-switching)
  # For English conversations:
  coqui_model: "tts_models/en/ljspeech/tacotron2-DDC"  # English only, 113MB
  # For Chinese conversations (uncomment below, comment out English above):
  # coqui_model: "tts_models/zh-CN/baker/tacotron2-DDC-GST"  # Chinese only, 686MB
  
  coqui_device: "auto"  # auto, cuda, cpu
  
  # Fallback pyttsx3 settings
  voice_id: null  # null for default
  rate: 180  # words per minute
  volume: 1.0
  
  # Language settings
  default_language: "auto"  # auto-detect, or specify: en, zh, zh-cn, es, fr, etc.
  
llm:
  provider: "llama.cpp"
  base_url: "http://localhost:8080"
  model: "llama-3.2-3b-instruct"
  binary_path: "/home/qing/Project/llama.cpp/build/bin/llama-server"
  max_tokens: 512
  temperature: 0.7
  system_prompt: |
    You are Talkie, a helpful voice assistant. You communicate through voice, so keep responses concise and natural.
    
    You have access to tools that allow you to:
    - Listen to user speech (already done for you)
    - Speak responses aloud
    - Check the weather
    - Execute system commands
    - Read and write files
    - Search the web for current information
    - Get latest news on any topic
    
    IMPORTANT: When the user asks about current events, news, or information that might not be in your training data, USE the web_search or web_news tools to find current information. Do not guess or make up information about current events.
    
    When you need to respond verbally, use the 'speak' tool.
    When you need information from the user, ask them to speak.
    
    Be friendly, helpful, and efficient.

mcp:
  server_name: "talkie-server"
  tools_enabled:
    - listen
    - speak
    - weather
    - execute_command
    - read_file
    - write_file
    - list_directory
    - wake_word
    - voice_activity
    - timer
    - calculator
    - web_search
    - web_news
    
weather:
  api_key: null  # Get from OpenWeatherMap or similar
  default_city: "San Francisco"
  units: "metric"  # metric or imperial

wake_word:
  phrases:
    - "hey talkie"
    - "ok talkie"
    - "talkie"
  simulation_mode: true  # Text-based simulation when no microphone
  sensitivity: 0.7  # Detection sensitivity (0.0 - 1.0)

vad:
  energy_threshold: 300  # Voice activity detection threshold
  silence_timeout: 2.0  # Seconds of silence before stopping recording
