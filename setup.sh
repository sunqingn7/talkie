#!/bin/bash

# Talkie Voice Assistant Setup Script
# MCP-First Design with whisper.cpp

set -e

echo "üöÄ Setting up Talkie Voice Assistant..."
echo "=========================================="

# Check Python version
echo "üì¶ Checking Python version..."
python_version=$(python3 --version 2>&1 | awk '{print $2}')
echo "   Found Python $python_version"

# Create virtual environment
echo "üì¶ Creating virtual environment..."
if [ ! -d "venv" ]; then
    python3 -m venv venv
    echo "   ‚úì Virtual environment created"
else
    echo "   ‚úì Virtual environment already exists"
fi

# Activate virtual environment
echo "üì¶ Activating virtual environment..."
source venv/bin/activate

# Upgrade pip
echo "üì¶ Upgrading pip..."
pip install --upgrade pip

# Install Python dependencies
echo "üì¶ Installing Python dependencies..."
pip install -r requirements.txt

# Create models directory
echo "üì¶ Setting up models directory..."
mkdir -p models

# Check whisper.cpp binary
echo ""
echo "üì¶ Checking whisper.cpp..."
WHISPER_BIN="/home/qing/Project/whisper.cpp/build/bin/whisper-server"
if [ -f "$WHISPER_BIN" ]; then
    echo "   ‚úì whisper.cpp found at: $WHISPER_BIN"
else
    echo "   ‚ùå whisper.cpp not found at: $WHISPER_BIN"
    echo "   Please build whisper.cpp first:"
    echo "     cd /home/qing/Project/whisper.cpp"
    echo "     mkdir -p build && cd build"
    echo "     cmake .. && make"
    exit 1
fi

# Check llama.cpp binary
echo ""
echo "üì¶ Checking llama.cpp..."
LLAMA_BIN="/home/qing/Project/llama.cpp/build/bin/llama-server"
if [ -f "$LLAMA_BIN" ]; then
    echo "   ‚úì llama.cpp found at: $LLAMA_BIN"
else
    echo "   ‚ùå llama.cpp not found at: $LLAMA_BIN"
    echo "   Please build llama.cpp first:"
    echo "     cd /home/qing/Project/llama.cpp"
    echo "     mkdir -p build && cd build"
    echo "     cmake .. && make"
    exit 1
fi

# Download whisper.cpp model if not exists
WHISPER_MODEL="models/ggml-base.en.bin"
if [ ! -f "$WHISPER_MODEL" ]; then
    echo ""
    echo "üì¶ Downloading whisper.cpp model..."
    echo "   This may take a few minutes..."
    
    # Download using wget or curl
    if command -v wget &> /dev/null; then
        wget -q --show-progress -O "$WHISPER_MODEL" \
            "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin"
    elif command -v curl &> /dev/null; then
        curl -L -o "$WHISPER_MODEL" \
            "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin"
    else
        echo "   ‚ö†Ô∏è  Please manually download whisper model from:"
        echo "      https://huggingface.co/ggerganov/whisper.cpp"
        echo "      Save to: $WHISPER_MODEL"
        exit 1
    fi
    
    echo "   ‚úì Downloaded whisper.cpp model"
else
    echo "   ‚úì whisper.cpp model already exists"
fi

# Check for Llama models in cache
echo ""
echo "üì¶ Checking for Llama models..."
LLAMA_CACHE_DIR="/home/qing/.cache/llama.cpp"
if [ -d "$LLAMA_CACHE_DIR" ]; then
    model_count=$(find "$LLAMA_CACHE_DIR" -name "*.gguf" -type f 2>/dev/null | wc -l)
    if [ "$model_count" -gt 0 ]; then
        echo "   ‚úì Found $model_count model(s) in cache"
        echo ""
        echo "   Available models:"
        find "$LLAMA_CACHE_DIR" -name "*.gguf" -type f -exec basename {} \; | while read -r model; do
            size=$(du -h "$LLAMA_CACHE_DIR/$model" | cut -f1)
            echo "     ‚Ä¢ $model ($size)"
        done
    else
        echo "   ‚ö†Ô∏è  No models found in cache"
        echo "   You can download models using:"
        echo "     llama-cli --hf-repo <repo> --model <model>"
    fi
else
    echo "   ‚ö†Ô∏è  Cache directory not found: $LLAMA_CACHE_DIR"
fi

# Setup complete
echo ""
echo "=========================================="
echo "‚úÖ Setup complete!"
echo ""
echo "üìù Next steps:"
echo "   1. Start the servers:"
echo "      ./start_servers.sh"
echo "      (You'll be prompted to select a model)"
echo ""
echo "   2. In a new terminal, run the assistant:"
echo "      source venv/bin/activate"
echo "      python src/main.py"
echo ""
