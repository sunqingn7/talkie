# Talkie Voice Assistant

ğŸ—£ï¸ An intelligent voice-powered AI assistant with web control panel

## âœ¨ Features

- **Voice Interface**: Speech-to-text (whisper.cpp) + Text-to-Speech
- **Multiple TTS Engines**: Qwen TTS (default), Edge TTS, Coqui XTTS, pyttsx3
- **16+ Voices**: English, Chinese, Japanese, Korean, Spanish, French, German, and more
- **Web Control Panel**: Real-time control via browser interface
- **File Reading**: Read uploaded files or webpages aloud with pause/resume
- **Smart Interruption**: Chat messages pause file reading, auto-resume after response
- **Multi-LLM Support**: Switch between vllm, llama.cpp, ollama, and cloud providers
- **Weather**: Automatic location detection from IP, works for any city worldwide
- **Web Search**: Integrated search via Tavily API / DuckDuckGo
- **14+ Built-in Tools**: Weather, calculator, timer, file operations, commands, etc.

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

Or install core dependencies:

```bash
pip install edge-tts tavily-python fastapi uvicorn websockets
pip install fast-whisper  # Or use whisper.cpp
pip install coqui-tts  # Optional, for local TTS
```

### 2. Configure

Copy the example config and update it:

```bash
cp config/settings.example.yaml config/settings.yaml

# Edit config/settings.yaml to add:
# - Tavily API key (get from https://tavily.com, free tier)
# - Paths to whisper.cpp and llama-server
# - Model paths
# - Custom settings
```

### 3. Run

**Web Interface (Recommended)**:
```bash
python web_server.py
# Open http://localhost:8082
```

**Command Line**:
```bash
python src/main.py
```

## ğŸ¨ Web Control Panel

Access at **http://localhost:8082**

- **Real-time Chat**: Interface for conversation with the assistant
- **TTS Engine Control**: Switch between Edge TTS, Coqui, and pyttsx3
- **Voice Selection**: 16+ voices for Edge TTS, XTTS personas for Coqui
- **LLM Management**: Switch between available models
- **System Status**: Monitor all tools and services

## ğŸŒ Recent Updates

### File Reading with Pause/Resume (Latest)
- **Chunk-by-Chunk Reading**: Progressive loading with buffer-based streaming
- **Smart Interruption**: Chat messages automatically pause file reading
- **Auto-Resume**: File reading resumes automatically after chat response
- **Position Persistence**: Resume from exact position across sessions
- **URL Support**: Read webpages directly by providing URL

### Multi-LLM Orchestrator
- **Provider Support**: vllm, llama.cpp, ollama, Google, Anthropic, xAI
- **Auto-Detection**: Automatic model detection for vllm and llama.cpp
- **Agent System**: Specialized agents (coder, reasoner, searcher, etc.)
- **Fallback System**: Automatic fallback to backup provider on failure
- **Dynamic Switching**: Switch providers via web interface or commands

### Qwen TTS Integration
- **Default TTS Engine**: Qwen/Qwen3-TTS-12Hz (high quality, fast)
- **Multilingual**: Excellent Chinese and English support
- **Auto Language Detection**: Automatically detects input language
- **Voice Design**: Custom voice characteristics support

### Weather Tool Upgrade
- **API**: Open-Meteo (free, no API key)
- **Features**:
  - Weather for any city worldwide
  - IP-based location auto-detection (ipinfo.io)
  - Temperature, humidity, wind speed, conditions
- **Smart Query Handling**: Auto-detects location if not provided

### System Improvements
- **Voice Daemon**: Centralized TTS queue with priority management
- **Priority-Based Speech**: HIGH (chat) vs NORMAL (file reading) priorities
- **Dynamic Audio Timeout**: Long responses play completely without cutoff
- **Sentence-Based Chunking**: Intelligent text splitting for natural pauses
- **Config Persistence**: All settings saved automatically
- **Enhanced Web Interface**: Real-time status, model switching, file upload

## ğŸ“ Configuration

### Key Settings (`config/settings.yaml`)

```yaml
tts:
  engine: qwen_tts  # Options: qwen_tts, edge_tts, coqui, pyttsx3
  voice_output: web  # Output to web interface (or 'local' for system audio)

llm:
  default_provider: llamacpp  # Options: vllm, llamacpp, ollama, google, etc.
  auto_detect_models: true  # Auto-detect available models

weather:
  api_key: null  # Optional: OpenWeatherMap API key
  auto_detect_location: true  # Auto-detect user location

web_search:
  tavily_api_key: null  # Required for web search
  use_duckduckgo_fallback: true
```

### TTS Engines

| Engine | Type | Description |
|--------|------|-------------|
| **qwen_tts** | Local (GPU) | Qwen3-TTS - default, high quality, fast |
| **edge_tts** | Online | Edge TTS - 16+ voices, no install |
| **coqui** | Local | XTTS-v2 - requires ~1.5GB download |
| **pyttsx3** | Local | Basic fallback TTS |

### LLM Providers

| Provider | Type | Description |
|----------|------|-------------|
| **vllm** | Local (GPU) | High-throughput inference server |
| **llamacpp** | Local (CPU/GPU) | llama.cpp backend, auto-detect models |
| **ollama** | Local | Ollama runtime support |
| **google** | Cloud | Google Gemini API |
| **anthropic** | Cloud | Claude API |
| **xAI** | Cloud | Grok API |

## ğŸ› ï¸ Available Tools

1. âœ… `listen` - Speech-to-text (whisper.cpp)
2. âœ… `speak` - Text-to-speech (Qwen TTS / Edge TTS / Coqui / pyttsx3)
3. âœ… `read_file_chunk` - Read files/webpages aloud with pause/resume
4. âœ… `pause_reading` - Pause current file reading
5. âœ… `resume_reading` - Resume paused file reading
6. âœ… `stop_reading` - Stop file reading completely
7. âœ… `weather` - Weather with IP auto-detection
8. âœ… `execute_command` - Run shell commands
9. âœ… `write_file` - Write files
10. âœ… `list_directory` - List directories
11. âœ… `wake_word` - Wake phrase detection
12. âœ… `voice_activity` - Voice activity detection
13. âœ… `timer` - Timer functionality
14. âœ… `calculator` - Math calculations
15. âœ… `web_search` - Web search
16. âœ… `datetime` - Current date/time

## ğŸ“ Usage Examples

### Voice Commands

```
"What's the weather?" â†’ Auto-detects location
"What's the weather in Tokyo?" â†’ Tokyo weather
"Read this file" â†’ Reads uploaded file aloud
"Pause reading" / "Resume reading" â†’ Control file reading
"Search for AI news" â†’ Web search
"Set a 5 minute timer"
"Calculate 15 times 23"
```

### File Reading

- **Upload a file** via web interface or mention it in chat
- **Read webpage**: Just provide a URL (e.g., "Read https://example.com")
- **Chat during reading**: File reading pauses automatically, resumes after response
- **Position saved**: Resume from exact position across sessions

### Web Interface

1. Start web server: `python web_server.py`
2. Open browser: `http://localhost:8082`
3. Use **Control Panel** to:
   - Switch TTS engines (Qwen, Edge, Coqui)
   - Switch LLM providers (vllm, llama.cpp, ollama, cloud)
   - Upload files for reading
   - Test voices
   - Monitor system status

## ğŸ”§ Setup Requirements

### Optional (Recommended for best experience)

- **whisper.cpp** - Local speech-to-text:
  ```bash
  # Clone and build whisper.cpp
  git clone https://github.com/ggerganov/whisper.cpp
  cd whisper.cpp
  make
  ```

- **llama.cpp** - LLM inference:
  ```bash
  # Clone and build llama.cpp
  git clone https://github.com/ggerganov/llama.cpp
  cd llama.cpp
  make
  ```

- **Coqui TTS** - Local high-quality TTS (optional):
  ```bash
  pip install TTS
  export COQUI_TOS_AGREED=1
  ```

### Required

- Python 3.10+
- FastAPI, uvicorn, websockets
- edge-tts (for default TTS)
- tavily-python (for web search)

## ğŸ“Š Project Structure

```
talkie/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ models.yaml        # LLM model configurations
â”‚   â”œâ”€â”€ settings.yaml      # Main configuration (not in git)
â”‚   â””â”€â”€ settings.example.yaml  # Example configuration template
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/             # Core LLM, model management, voice daemon
â”‚   â”‚   â”œâ”€â”€ llm_providers/  # Multi-provider LLM support
â”‚   â”‚   â”œâ”€â”€ voice_daemon.py # Priority-based TTS queue
â”‚   â”‚   â””â”€â”€ reading_position_manager.py  # Position persistence
â”‚   â”œâ”€â”€ mcp_integration/  # MCP server and tool registration
â”‚   â”œâ”€â”€ tools/            # All available tools
â”‚   â”‚   â”œâ”€â”€ qwen_tts_tool.py     # Qwen TTS (default)
â”‚   â”‚   â”œâ”€â”€ edge_tts_tool.py     # Edge TTS
â”‚   â”‚   â”œâ”€â”€ tts_tool.py          # TTS manager
â”‚   â”‚   â”œâ”€â”€ file_reading_tool.py # File reading with pause/resume
â”‚   â”‚   â””â”€â”€ web_fetch_tool.py    # Webpage fetching
â”‚   â”œâ”€â”€ utils/            # Utilities
â”‚   â”‚   â””â”€â”€ file_stream_reader.py # Streaming file reader
â”‚   â””â”€â”€ web/              # Web interface
â”‚       â”œâ”€â”€ web_server.py
â”‚       â”œâ”€â”€ templates/
â”‚       â”‚   â””â”€â”€ index.html
â”‚       â””â”€â”€ static/
â”‚           â”œâ”€â”€ css/
â”‚           â”‚   â””â”€â”€ style.css
â”‚           â””â”€â”€ js/
â”‚               â””â”€â”€ app.js
â””â”€â”€ requirements.txt
```

## ğŸ¯ Use Cases

- **Personal Voice Assistant**: Hands-free information access
- **Smart Home Control**: Connect to IoT devices via commands
- **Accessibility**: Voice interface for accessibility needs
- **Education**: Interactive learning assistant
- **Demo Platform**: Show off AI capabilities

## ğŸ” Security & Privacy

- All sensitive data is in `config/settings.yaml` (excluded from git)
- Use provided `config/settings.example.yaml` as template
- No telemetry or data collection
- Runs locally on your machine

## ğŸ“„ License

This project is open source and available under an appropriate license.

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- More TTS engines and voices
- Additional tools and features
- Mobile responsiveness improvements
- User interface enhancements
- Integration with other services

## ğŸ“ Notes

- **Qwen TTS** requires GPU for best performance (CUDA recommended)
- **Edge TTS** requires internet connection (online service)
- **Coqui TTS** can work offline but requires ~1.5GB download
- **vllm/llama.cpp** require compatible GGUF or HF models
- **Weather** uses ipinfo.io for location detection (free, no signup needed)
- **Web Search** requires Tavily API key (free tier: 1000 calls/month)
- **File Reading** works with any text file or webpage URL

---

**Built with â¤ï¸ using Python, FastAPI, Qwen TTS, and Multi-LLM Orchestrator**

**Latest**: Commit `13b9dc0` - Pause/resume file reading when chat arrives
